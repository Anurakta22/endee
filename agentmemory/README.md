---
title: EndeeAgent Memory
emoji: ğŸ§ 
colorFrom: blue
colorTo: purple
sdk: docker
pinned: false
---

# ğŸ§  AgentMemory> **Long-term episodic memory for AI agents, powered by [Endee](https://github.com/endee-io/endee) vector database.**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Endee](https://img.shields.io/badge/vector--db-Endee-brightgreen)](https://github.com/endee-io/endee)
[![FastAPI](https://img.shields.io/badge/API-FastAPI-009688)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/docker-required-blue)](https://www.docker.com/)
[![Tests](https://img.shields.io/badge/tests-pytest-informational)](tests/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“Œ Project Overview & Problem Statement

Modern AI assistants are **stateless by default**. Every new conversation starts from a blank slate â€” the agent has no memory of who you are, what you've built together, or what preferences you've expressed before.

This means:
- You re-introduce yourself every session ("I'm Alex, I work in fintechâ€¦")
- The agent forgets your tech stack, coding style, and ongoing projects
- No personalisation across sessions

**AgentMemory solves this** by giving any AI agent a persistent, searchable long-term memory backed by **Endee**, a high-performance vector database capable of handling **up to 1 billion vectors on a single node**.

When you ask the agent "How do I set up async DB access?", it silently recalls from Endee that you are a senior engineer building a FastAPI + PostgreSQL backend, that you prefer concise code examples, and that your deadline is end of March â€” and responds accordingly, without you having to repeat any of that.

---

## ğŸ—ï¸ System Design & Technical Approach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AgentMemory Architecture                        â”‚
â”‚                                                                        â”‚
â”‚  USER MESSAGE                                                          â”‚
â”‚       â”‚                                                                â”‚
â”‚       â–¼                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  embed()  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  Embedder   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Endee Index    â”‚  index.query()        â”‚
â”‚  â”‚(MiniLM-L6)  â”‚           â”‚  (cosine, INT8)  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚            â”‚
â”‚                                      â–²                    â–¼            â”‚
â”‚                               upsert â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                                      â”‚         â”‚ Top-K Memories  â”‚    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  summarise  â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”‚  LLM Buffer â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ MemoryStore  â”‚           â”‚            â”‚
â”‚  â”‚(session buf)â”‚             â”‚ (Endee wrap) â”‚           â–¼            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚         â–²                                       â”‚  LLM (Mistral)    â”‚   â”‚
â”‚         â”‚                                       â”‚  + context      â”‚   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚            â”‚
â”‚                                                    ANSWER + SOURCES   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Lifecycle

| Phase | What Happens | Endee Operation |
|-------|-------------|-----------------|
| **Recall** | User sends a message â†’ embed it â†’ find similar past memories | `index.query(vector, top_k=5)` |
| **Generate** | Inject memories into LLM system prompt â†’ produce grounded answer | â€” |
| **Buffer** | Append (user, assistant) turns to session buffer | â€” |
| **Checkpoint** | Every N turns â†’ LLM summarises buffer â†’ save to Endee | `index.upsert([{id, vector, meta}])` |
| **Persist** | On SIGINT / session end â†’ flush remaining buffer | `index.upsert(...)` |

### Why this architecture works

- **Semantic, not keyword**: Endee's ANN search finds memories by meaning, not exact words. "How do I handle async DB?" will recall "User is building FastAPI + PostgreSQL" even though neither "async" nor "DB" appears verbatim.
- **Scales indefinitely**: Endee handles 1B+ vectors; your agent can accumulate years of memory.
- **Summarisation before storage**: The LLM distills conversations into compact, information-dense memories before embedding â€” reducing noise and storage costs.
- **Cross-session recall**: Memories from any past session are searchable, enabling true long-term personalisation.

---

## ğŸ§© How Endee Is Used

AgentMemory uses the **official Endee Python SDK** (`pip install endee`) as its sole vector store. All vector operations go through Endee.

### Index Configuration

```python
from endee import Endee, Precision

client = Endee()  # connects to http://localhost:8080 by default
client.create_index(
    name="agent_memory",
    dimension=384,           # all-MiniLM-L6-v2 embedding size
    space_type="cosine",     # cosine similarity (normalised dot-product)
    precision=Precision.INT8 # INT8 quantisation: 4Ã— smaller, ~2Ã— faster
)
```

### Storing a Memory (Upsert)

```python
index = client.get_index("agent_memory")
index.upsert([
    {
        "id": "sess_abc_1706345600000_d3f1a2",
        "vector": [0.12, -0.34, ...],   # 384-dim embedding of the summary
        "meta": {
            "session_id":  "sess_abc",
            "summary":     "User prefers dark mode; asked about React hooks.",
            "role":        "mixed",
            "turn":        12,
            "tags":        ["preference", "react"],
            "timestamp":   "2026-02-27T10:30:00Z"
        }
    }
])
```

### Recalling Memories (Query)

```python
query_vector = embed("What UI preferences does the user have?")  # 384 floats

results = index.query(vector=query_vector, top_k=5)
# results[i].id          â†’ memory ID
# results[i].similarity  â†’ cosine similarity score (0â€“1)
# results[i].meta        â†’ full metadata dict including summary text
```

The `summary` text from each result is assembled into a context block and injected into the LLM's system prompt before answering.

### Why Endee?

| Requirement | How Endee Meets It |
|-------------|-------------------|
| Fast ANN search (< 5 ms at 1M vectors) | SIMD-optimised HNSW (AVX2/AVX512/NEON/SVE2) |
| Single-node scalability | Up to 1B vectors per node |
| Simple integration | Official Python SDK + REST API |
| Persistent storage across restarts | Docker volume (`endee-data`) |
| Low operational overhead | Single Docker container |
| Open source | Apache-2.0 licence |

---

## ğŸ“ Project Structure

```
agentmemory/
â”œâ”€â”€ docker-compose.yml       â† Starts Endee (endeeio/endee-server:latest)
â”œâ”€â”€ .env.example             â† Configuration template
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py            â† Centralised settings (reads .env)
â”‚   â”œâ”€â”€ embedder.py          â† sentence-transformers wrapper (384-dim vectors)
â”‚   â”œâ”€â”€ memory_store.py      â† MemoryEntry + MemoryStore (Endee SDK wrapper) â­
â”‚   â”œâ”€â”€ summariser.py        â† LLM summarisation & answer generation
â”‚   â”œâ”€â”€ agent.py             â† MemoryAgent orchestrator
â”‚   â”œâ”€â”€ cli.py               â† Rich interactive REPL
â”‚   â”œâ”€â”€ api.py               â† FastAPI HTTP server
â”‚   â””â”€â”€ __main__.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ demo.py              â† Standalone demo (no LLM key needed)
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_embedder.py
    â”œâ”€â”€ test_memory_store.py
    â”œâ”€â”€ test_agent.py
    â””â”€â”€ test_api.py
```

---

## ğŸš€ Setup & Execution Instructions

### Prerequisites

| Tool | Version | Purpose |
|------|---------|---------|
| Docker + Docker Compose | v2+ | Run Endee vector DB |
| Python | 3.10+ | Run AgentMemory |
| Mistral API key | â€” | LLM for answers & summarisation |

---

### Step 1 â€” Start Endee

```bash
docker compose up -d
```

Endee will start on port `8080`. Verify it's running:

```bash
curl http://localhost:8080/api/v1/index/list
# â†’ {"indexes":[]}
```

Or open [http://localhost:8080](http://localhost:8080) in your browser to access the Endee dashboard.

---

### Step 2 â€” Install Python Dependencies

```bash
python -m venv .venv
source .venv/bin/activate       # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

---

### Step 3 â€” Configure Environment

```bash
cp .env.example .env
```

Edit `.env`:

```env
# Required for LLM
LLM_PROVIDER=mistral
MISTRAL_API_KEY=YOUR_MISTRAL_API_KEY
MISTRAL_MODEL=mistral-large-latest
```

Everything else works with defaults for local development.

---

### Step 4 â€” Run the Demo (no LLM key needed)

The demo seeds pre-written memories into Endee and demonstrates semantic recall without an LLM call:

```bash
python scripts/demo.py
```

Expected output:

```
AgentMemory â€“ Demo

â‘  Seeding past memories into Endee...
âœ“ Seeded 6 memories into Endee.

â‘¡ Demonstrating semantic recall from Endee...

Query: What UI preferences does the user have?
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rankâ”‚ Retrieved Memory                                      â”‚ Tags                â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ #1  â”‚ The user prefers dark mode across all applications... â”‚ preference, ui, ... â”‚
â”‚ #2  â”‚ Alex prefers concise code examples over lengthy...    â”‚ preference, coding  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
...

â‘¢ Simulating a conversation with memory injection...
```

---

### Step 5 â€” Interactive Chat (requires LLM key)

```bash
python -m src.cli
```

**Resume a named session** (memories persist across restarts):

```bash
python -m src.cli --session my_project_sess
```

**In-chat commands:**

| Command | Description |
|---------|-------------|
| `/recall <query>` | Search Endee memories semantically |
| `/history` | Show all memories from this session |
| `/save` | Force checkpoint to Endee |
| `/stats` | Show Endee index statistics |
| `exit` | Quit and auto-save session |

**Example session:**

```
AgentMemory â€” AI agent with long-term episodic memory
Vector DB: Endee  â”‚  Session: sess_f3a1b2c4

You: My name is Alex and I'm building a FastAPI app with PostgreSQL
Agent: Nice to meet you, Alex! FastAPI + PostgreSQL is a great combination...

You: I prefer dark mode and concise code examples
Agent: Got it â€” I'll keep examples tight and minimal going forward.

[... 20 turns later, memories auto-saved to Endee ...]

# â”€â”€ New process, same session â”€â”€
$ python -m src.cli --session sess_f3a1b2c4

You: Help me with my database setup
Agent: Sure Alex! Since you're working on FastAPI + PostgreSQL and prefer
       concise examples, here's what you need...
       ^ Remembers your name, stack, and preferences from the previous session
```

---

### Step 6 â€” REST API Server

```bash
uvicorn src.api:app --reload --port 7860
```

Open [http://localhost:7860/docs](http://localhost:7860/docs) for the interactive Swagger UI.

**Key endpoints:**

```bash
# Create / resume a session
curl -X POST http://localhost:7860/sessions \
  -H "Content-Type: application/json" \
  -d '{"session_id": "my_session"}'

# Chat
curl -X POST http://localhost:7860/sessions/my_session/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What frameworks should I use for my project?"}'

# Semantic memory search
curl -X POST http://localhost:7860/memories/search \
  -H "Content-Type: application/json" \
  -d '{"query": "user programming preferences", "top_k": 5}'

# View session memories
curl http://localhost:7860/sessions/my_session/memories
```

---

### Step 7 â€” Run Tests

```bash
pytest tests/ -v
```

All tests run offline (Endee and LLM calls are mocked).

---

## ğŸ”§ Configuration Reference

| Variable | Default | Description |
|----------|---------|-------------|
| `ENDEE_BASE_URL` | `http://localhost:8080` | Endee server address |
| `ENDEE_AUTH_TOKEN` | *(empty)* | Auth token (optional for local dev) |
| `ENDEE_INDEX_NAME` | `agent_memory` | Name of the Endee index |
| `LLM_PROVIDER` | `mistral` | `mistral` |
| `MISTRAL_API_KEY` | â€” | Your Mistral key |
| `MISTRAL_MODEL` | `mistral-large-latest` | Mistral model name |
| `EMBED_MODEL` | `all-MiniLM-L6-v2` | Sentence-transformers model |
| `MEMORY_TOP_K` | `5` | Memories retrieved per query |
| `SUMMARY_MAX_TOKENS` | `150` | Max tokens per memory summary |
| `SESSION_WINDOW` | `20` | Turns before auto-checkpoint |

---

## ğŸ“ˆ Performance Characteristics

| Metric | Value |
|--------|-------|
| Embedding latency (CPU) | ~20 ms per text |
| Endee upsert (1 memory) | < 5 ms |
| Endee query (1M vectors) | < 5 ms (SIMD-optimised HNSW) |
| Memory per vector (INT8, 384-dim) | ~400 bytes |
| Max vectors (single node) | 1,000,000,000 |
| End-to-end turn latency | ~1â€“3 s (LLM-dominated) |

---

## â˜ï¸ Free Deployment (Hugging Face Spaces)

You can host AgentMemory 100% for free on **Hugging Face Spaces**, which automatically spins up both the Endee Vector Database and the FastAPI server within a single Docker container.

1. Create a new [Hugging Face Space](https://huggingface.co/new-space).
2. Choose **Docker** as the Space SDK and select the **Blank** template.
3. Link your GitHub repository to your Hugging Face Space (or push the code manually).
4. In your Hugging Face Space settings, add your `MISTRAL_API_KEY` under **Variables and secrets** (as a Secret).
5. The included `Dockerfile` and `start.sh` script will automatically:
   - Start the Endee Vector Database in the background (`localhost:8080`).
   - Start the FastAPI web server on Hugging Face's required port (`7860`).
6. Once the build finishes, your API will be live!

---

## ğŸ› ï¸ Production Notes

- Set `NDD_AUTH_TOKEN` in both `docker-compose.yml` and `.env` for authenticated access
- For high-throughput: increase `NDD_NUM_THREADS` in `docker-compose.yml`
- Back up Endee data with: `docker run --rm -v endee-data:/data -v $(pwd):/backup alpine tar czf /backup/endee-backup.tar.gz /data`
- Use AVX512 build on server CPUs (Intel Xeon / AMD EPYC) for maximum speed

---

## ğŸ¤ Contributing

Pull requests are welcome. Please open an issue first for major changes.

---

## ğŸ“„ License

MIT Â© 2026
